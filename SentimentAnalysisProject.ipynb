{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This project requests data from the reddit api. Converts text to tsvectors and moved into a sql database to be queried similar to a search engine. The text is also analyzed for sentiment analysis and visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the latest version of praw\n",
    "pip install --upgrade https://github.com/praw-dev/praw/archive/master.zip"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Load in packages and verify credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load praw and password info saved on a py notebook\n",
    "import praw\n",
    "import reddit_info\n",
    "\n",
    "reddit = praw.Reddit(client_id= reddit_info.client_id, \n",
    "                     client_secret= reddit_info.client_secret, \n",
    "                     user_agent= reddit_info.username)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "# Initialize some variables\n",
    "mysso= 'sebmb'  #student id is also schema name\n",
    "schema='sebmb' \n",
    "hostname='pgsql.dsa.lan'\n",
    "database='dsa_student'\n",
    "\n",
    "mypasswd = getpass.getpass(\"Type Password and hit enter\")\n",
    "connection_string = f\"postgres://{mysso}:{mypasswd}@{hostname}/{database}\"\n",
    "\n",
    "%load_ext sql\n",
    "%sql $connection_string \n",
    "\n",
    "# Then remove the password from computer memory\n",
    "del mypasswd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "create the table now to load the data into later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "DROP TABLE IF EXISTS subreddit;\n",
    "\n",
    "CREATE TABLE subreddit(\n",
    "        id SERIAL NOT NULL PRIMARY KEY,\n",
    "        title  VARCHAR(500) NOT NULL,\n",
    "        title_link VARCHAR(500) NOT NULL,\n",
    "        author  VARCHAR(500),\n",
    "        subreddit  VARCHAR(500) NOT NULL,\n",
    "        tags VARCHAR(500),\n",
    "        time_stamp DATE NOT NULL,\n",
    "        line text NOT NULL,\n",
    "        score INT,\n",
    "        sentiment VARCHAR(500)\n",
    "\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "--add in the tsvector column\n",
    "\n",
    "ALTER TABLE subreddit\n",
    "    ADD COLUMN line_tsv_gin tsvector;\n",
    "    \n",
    "UPDATE subreddit\n",
    "SET line_tsv_gin = to_tsvector('pg_catalog.english', line);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "add trigger to auto update text into tsvector when the data is loaded in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Add triggers\n",
    "\n",
    "DROP TRIGGER IF EXISTS tsv_gin_update on subreddit;\n",
    "\n",
    "CREATE TRIGGER tsv_gin_update \n",
    "    BEFORE INSERT OR UPDATE\n",
    "    ON subreddit \n",
    "    FOR EACH ROW \n",
    "    EXECUTE PROCEDURE \n",
    "    tsvector_update_trigger(line_tsv_gin,'pg_catalog.english',line);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Add indexes\n",
    "\n",
    "-- Index on content (Trigram needed,to use Gin Index)\n",
    "-- CREATE EXTENSION pg_trgm;  -- Done by DB Admin\n",
    "\n",
    "CREATE INDEX subreddit_line\n",
    "ON subreddit USING GIN(line gin_trgm_ops);\n",
    "\n",
    "-- GIN INDEX on line_tsv_gin\n",
    "CREATE INDEX subreddit_line_tsv_gin\n",
    "ON subreddit USING GIN(line_tsv_gin);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the 1000 latest posts to use for this project\n",
    "latest_posts = reddit.subreddit('leagueoflegends').new(limit=1000)  # newest posts\n",
    "all_posts = list(latest_posts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove posts that do not have text\n",
    "#initalize lists and append items we want from each post to a list\n",
    "#create a df containing each portion of the post as a column\n",
    "title = []\n",
    "author = []\n",
    "subreddit = []\n",
    "tags = []\n",
    "title_link = []\n",
    "time_stamp = []\n",
    "line = []\n",
    "score = []\n",
    "\n",
    "\n",
    "for post in all_posts:\n",
    "    if post.selftext == '':\n",
    "        continue\n",
    "    else:\n",
    "        title.append(post.title)\n",
    "        title_link.append(post.url)\n",
    "        author.append(post.author)\n",
    "        subreddit.append(post.subreddit)\n",
    "        tags.append(post.link_flair_text)\n",
    "        time_stamp.append(post.created)\n",
    "        line.append(post.selftext)\n",
    "        score.append(post.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure each post was scraped correctly by checking length of lists\n",
    "print(len(title))\n",
    "print(len(line))\n",
    "print(len(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a df of the lists\n",
    "import pandas as pd\n",
    "\n",
    "sub_df = pd.DataFrame(title, columns = ['title'])\n",
    "sub_df['title_link'] = title_link\n",
    "sub_df['author'] = author\n",
    "sub_df['subreddit'] = subreddit\n",
    "sub_df['tags'] = tags\n",
    "sub_df['time_stamp'] = time_stamp\n",
    "sub_df['line'] = line\n",
    "sub_df['score'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get more subreddits into df\n",
    "\n",
    "#get the 1000 latest posts to use for this project\n",
    "latest_posts = reddit.subreddit('leaguepbe').new(limit=1000)  # newest posts\n",
    "all_posts = list(latest_posts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove posts that do not have text\n",
    "#initalize lists and append items we want from each post to a list\n",
    "#create a df containing each portion of the post as a column\n",
    "title = []\n",
    "author = []\n",
    "subreddit = []\n",
    "tags = []\n",
    "title_link = []\n",
    "time_stamp = []\n",
    "line = []\n",
    "score = []\n",
    "\n",
    "\n",
    "for post in all_posts:\n",
    "    if post.selftext == '':\n",
    "        continue\n",
    "    else:\n",
    "        title.append(post.title)\n",
    "        title_link.append(post.url)\n",
    "        author.append(post.author)\n",
    "        subreddit.append(post.subreddit)\n",
    "        tags.append(post.link_flair_text)\n",
    "        time_stamp.append(post.created)\n",
    "        line.append(post.selftext)\n",
    "        score.append(post.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure each post was scraped correctly by checking length of lists\n",
    "print(len(title))\n",
    "print(len(line))\n",
    "print(len(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a df of the lists\n",
    "\n",
    "leaguepbe = pd.DataFrame(title, columns = ['title'])\n",
    "leaguepbe['title_link'] = title_link\n",
    "leaguepbe['author'] = author\n",
    "leaguepbe['subreddit'] = subreddit\n",
    "leaguepbe['tags'] = tags\n",
    "leaguepbe['time_stamp'] = time_stamp\n",
    "leaguepbe['line'] = line\n",
    "leaguepbe['score'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.concat([sub_df,leaguepbe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get more subreddits into df\n",
    "\n",
    "#get the 1000 latest posts to use for this project\n",
    "latest_posts = reddit.subreddit('ornnmains').new(limit=1000)  # newest posts\n",
    "all_posts = list(latest_posts) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove posts that do not have text\n",
    "#initalize lists and append items we want from each post to a list\n",
    "#create a df containing each portion of the post as a column\n",
    "title = []\n",
    "author = []\n",
    "subreddit = []\n",
    "tags = []\n",
    "title_link = []\n",
    "time_stamp = []\n",
    "line = []\n",
    "score = []\n",
    "\n",
    "\n",
    "for post in all_posts:\n",
    "    if post.selftext == '':\n",
    "        continue\n",
    "    else:\n",
    "        title.append(post.title)\n",
    "        title_link.append(post.url)\n",
    "        author.append(post.author)\n",
    "        subreddit.append(post.subreddit)\n",
    "        tags.append(post.link_flair_text)\n",
    "        time_stamp.append(post.created)\n",
    "        line.append(post.selftext)\n",
    "        score.append(post.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure each post was scraped correctly by checking length of lists\n",
    "print(len(title))\n",
    "print(len(line))\n",
    "print(len(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a df of the lists\n",
    "\n",
    "ornn = pd.DataFrame(title, columns = ['title'])\n",
    "ornn['title_link'] = title_link\n",
    "ornn['author'] = author\n",
    "ornn['subreddit'] = subreddit\n",
    "ornn['tags'] = tags\n",
    "ornn['time_stamp'] = time_stamp\n",
    "ornn['line'] = line\n",
    "ornn['score'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.concat([sub_df,ornn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = sub_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have 3 subreddits collected and its time to perform sentiment analysis on the df\n",
    "\n",
    "lines = sub_df['line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the packages we are using in this project\n",
    "import nltk, re, pprint\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist\n",
    " \n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "reddit_sentiment = [analyzer.polarity_scores(t) for t in lines]\n",
    "\n",
    "sent = pd.DataFrame(reddit_sentiment)\n",
    "\n",
    "#add back lines for each sentiment\n",
    "sent['line'] = lines\n",
    "\n",
    "sent = sent[['line', 'neg', 'neu', 'pos', 'compound']]\n",
    "\n",
    "sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide the sentiment into negative,positive, or neutral categories\n",
    "sent['sentiment'] = 'NEU'\n",
    "sent.loc[sent['compound'] > 0.05, 'sentiment'] = 'POS'\n",
    "sent.loc[sent['compound'] < -0.05, 'sentiment'] = 'NEG'\n",
    "\n",
    "sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.boxplot(x=\"sentiment\", y=\"compound\", data=sent);\n",
    "#almost all positive and negative with a small amount of neutral posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#161 neutral posts out of 2113\n",
    "len(sent[sent['sentiment'] == 'NEU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#645 negative posts out of 2113\n",
    "len(sent[sent['sentiment'] == 'NEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1306 positive posts out of 2113\n",
    "len(sent[sent['sentiment'] == 'POS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge sentiment back to the df containing the other reddit data\n",
    "sub_df = sub_df.merge(sent, on='line', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as csv so scraped df can be reproduced\n",
    "reddit_df = pd.read_csv('sub_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove added columns \n",
    "reddit_df = reddit_df.drop(['Unnamed: 0', 'index'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert timestamp to utc\n",
    "import time\n",
    "from datetime import datetime as dt, timezone\n",
    "\n",
    "time = []\n",
    "\n",
    "for each in range(0,len(reddit_df)):\n",
    "    \n",
    "     time.append(dt.fromtimestamp(reddit_df['time_stamp'][each]))\n",
    "    \n",
    "    \n",
    "reddit_df['time_stamp'] = time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep data to be loaded \n",
    "reddit_df = reddit_df.drop(['neg', 'neu', 'pos', 'compound'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "mypasswd = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use psycopg to insert data from df into \n",
    "import psycopg2\n",
    "import numpy as np\n",
    "from psycopg2.extensions import adapt, register_adapter, AsIs\n",
    "\n",
    "connection = psycopg2.connect(database = 'dsa_student', \n",
    "                              user = 'sebmb', \n",
    "                              host = 'pgsql.dsa.lan',\n",
    "                              password = mypasswd)\n",
    "\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mypasswd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print %s for each column to be loaded\n",
    "print(list(reddit_df))\n",
    "s = ''\n",
    "for i in list(reddit_df):\n",
    "    s+= '%s,'\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert sql statement to initialize the columns \n",
    "register_adapter(np.int64, AsIs)\n",
    "\n",
    "reddit_df = reddit_df.where(pd.notnull(reddit_df), None)\n",
    "\n",
    "INSERT_SQL = 'INSERT INTO sebmb.subreddit'\n",
    "INSERT_SQL += '(title, title_link, author, subreddit, tags, time_stamp, line, score, sentiment) VALUES '\n",
    "INSERT_SQL += '(%s,%s,%s,%s,%s,%s,%s,%s,%s)'\n",
    "\n",
    "print(INSERT_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through the dataframe and insert the data in each column\n",
    "with connection, connection.cursor() as cursor:\n",
    "    for row in reddit_df.itertuples(index=False, name=None) :\n",
    "        \n",
    "        cursor.execute(INSERT_SQL,row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give read only privileges to the dsa_student database account \n",
    "%load_ext sql\n",
    "%sql postgres://dsa_ro_user:readonly@pgsql.dsa.lan/dsa_student\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "run some simple queries to see the database querying text with a ranked value similar to a search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT id, title, author, subreddit, line, sentiment, ts_rank_cd(line_tsv_gin, query) AS rank\n",
    "FROM sebmb.subreddit, to_tsquery('ornn') query\n",
    "WHERE query @@ line_tsv_gin\n",
    "ORDER BY rank DESC LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT id, title, author, subreddit, line, sentiment, ts_rank_cd(line_tsv_gin, query) AS rank\n",
    "FROM sebmb.subreddit, to_tsquery('worlds & tickets') query\n",
    "WHERE query @@ line_tsv_gin\n",
    "ORDER BY rank DESC LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT id, title, author, subreddit, line, sentiment, ts_rank_cd(line_tsv_gin, query) AS rank\n",
    "FROM sebmb.subreddit, to_tsquery('worlds') query\n",
    "WHERE query @@ line_tsv_gin\n",
    "ORDER BY rank DESC LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT id, title, author, subreddit, line, sentiment, ts_rank_cd(line_tsv_gin, query) AS rank\n",
    "FROM sebmb.subreddit, to_tsquery('riot') query\n",
    "WHERE query @@ line_tsv_gin\n",
    "ORDER BY rank DESC LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * \n",
    "FROM sebmb.subreddit \n",
    "WHERE author = 'RiotTriptoid';"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we know the database works we can get back to sentiment analysis with some visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grouped_subs = reddit_df.groupby(['subreddit','sentiment']).count()\n",
    "grouped_subs = grouped_subs.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get an idea of which subreddits have positive and negative comments\n",
    "\n",
    "sent_by_sub = sns.barplot(x='subreddit', y = 'title', hue = 'sentiment', data = grouped_subs)\n",
    "sent_by_sub.set(xlabel='subreddit', ylabel='count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take time stamp and divide the posts up by week\n",
    "reddit_df['WeekStartingMonday'] = reddit_df['time_stamp'].dt.isocalendar().week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a visual of the subreddits sentiment by week \n",
    "df_weekly = (reddit_df.groupby(['WeekStartingMonday', 'subreddit'], as_index=False)\n",
    "            .agg(countofposts= ('sent_rating', 'count'), sumofpoints = ('score', 'sum'), \\\n",
    "                 sumofcompound=('compound', 'sum'), sumofsentiment=('sent_rating', 'sum')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove league of legends since all 1000 posts were on the same day which isn't helpful for a time series plot\n",
    "df_weekly_trim = df_weekly[df_weekly['subreddit']!='leagueoflegends']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot weekly interaction and sentiment by week for each subreddit\n",
    "\n",
    "sentiment_time = sns.lineplot(x = \"WeekStartingMonday\", y = \"sumofcompound\", hue = 'subreddit',\n",
    "             data = df_weekly_trim)\n",
    "sentiment_time.axhline(0, color = 'r')  \n",
    "plt.xticks(rotation = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets view how much interaction each week has using points as a measure of interaction for each post\n",
    "\n",
    "point_time = sns.lineplot(x = \"WeekStartingMonday\", y = \"sumofpoints\", hue = 'subreddit',\n",
    "             data = df_weekly_trim)\n",
    "point_time.axhline(0, color = 'r')  \n",
    "plt.xticks(rotation = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets view points within that time frame that had interactions of 100 or greater and sort by most interactions\n",
    "high_interaction[high_interaction['score'] >= 100].sort_values('score', ascending = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Evaluation of my analysis:\n",
    "\n",
    "you can see that the highest interactions are during the star guardian event which was a high budget event promoted by league of legends where they made big changes to the shop, released new skins, and released animations for the event. they even brought in porter robinson to create a song for the event who is a very popular electronic music artist. \n",
    "\n",
    "this brough in peak interactions almost 3 times as much as any regular week for the company. the riot employees make feedback threads each week to respond to their community, and during this event the feedback threads garnered huge traction. \n",
    "\n",
    "the community seems to be overall happy with how league of legends has been performing as of recently. the big world event is this month and people seem to be posting positive news about the event and other changes being made. leaguepbe is where the community discusses bugs in the game and they seem to have positive replies as well. there is a larger ratio of negative to positive comments on that subreddit but that is due to the nature of changes in a game. people will be upset with bugs and also do not like change being implemented. leaguepbe still remains to be very highly positive. the last subreddit i examined was ornn mains because ornn is a pretty well designed champ within the game. i expected this subreddit to be smaller in comment size and highly positive because of ornn's good design. the comments would be smaller because the ornn playing community is smaller than the overall league community. \n",
    "\n",
    "Overall riot must be doing a pretty good job recently to not have these reddit pages up in arms. i expect leaguepbe and ornn mains to move in a way that the actual leagueoflegends subreddit moves. if people are enjoying the game, there should be positive comments across all three subreddits. however, this sentiment analysis could look very different when something is wrong with the game for a long period of time. people might take to reddit to voice complaints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
